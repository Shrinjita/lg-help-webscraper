{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shrinjita/lg-help-webscraper/blob/main/WebscrapingProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TycELY1dnrd",
        "outputId": "7fcb5d38-409f-4073-c351-fe8cf836678e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4CVa0y0HRQMD",
        "outputId": "29f292da-ddb2-4984-8bca-34822849799f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-colab-selenium in /usr/local/lib/python3.11/dist-packages (1.0.14)\n",
            "Requirement already satisfied: selenium in /usr/local/lib/python3.11/dist-packages (from google-colab-selenium) (4.28.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium->google-colab-selenium) (2.3.0)\n",
            "Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.11/dist-packages (from selenium->google-colab-selenium) (0.28.0)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.11/dist-packages (from selenium->google-colab-selenium) (0.11.1)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.11/dist-packages (from selenium->google-colab-selenium) (2024.12.14)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.11/dist-packages (from selenium->google-colab-selenium) (4.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.11/dist-packages (from selenium->google-colab-selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium->google-colab-selenium) (24.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium->google-colab-selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium->google-colab-selenium) (3.10)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium->google-colab-selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium->google-colab-selenium) (1.3.1)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.11/dist-packages (from trio-websocket~=0.9->selenium->google-colab-selenium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium->google-colab-selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium->google-colab-selenium) (0.14.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install google-colab-selenium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vM4aieH4RDLD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import time\n",
        "import json\n",
        "import csv\n",
        "import pandas as pd\n",
        "import requests\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "import google_colab_selenium as gs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBwx4OLURFCz"
      },
      "outputs": [],
      "source": [
        "# Initialize folder for the project\n",
        "project_folder = \"/content/drive/MyDrive/Colab Notebooks/WebscrapingProject\"\n",
        "os.makedirs(project_folder, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "KL2DEPBgdRbA",
        "outputId": "11b45901-13c0-416e-abea-f70b0ea32075"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing WebDriver...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "            <div class=\"spinner-container\">\n",
              "                <div class=\"spinner\" id=\"434c8b45-b178-4456-ae56-4ed00f158b24-circle\"></div>\n",
              "                <div class=\"spinner-text\" id=\"434c8b45-b178-4456-ae56-4ed00f158b24-text\">Initializing Chromedriver</div>\n",
              "            </div>\n",
              "            <style>\n",
              "                @keyframes spin {\n",
              "                    from { transform: rotate(0deg); }\n",
              "                    to { transform: rotate(360deg); }\n",
              "                }\n",
              "\n",
              "                .spinner-container {\n",
              "                    display: flex;\n",
              "                    align-items: center;\n",
              "                    margin-bottom: 3px;\n",
              "                }\n",
              "\n",
              "                .spinner {\n",
              "                    border: 3px solid rgba(0, 0, 0, 0.1);\n",
              "                    border-left-color: lightblue;\n",
              "                    border-radius: 50%;\n",
              "                    width: 12px;\n",
              "                    height: 12px;\n",
              "                    animation: spin 1s linear infinite;\n",
              "                }\n",
              "\n",
              "                .spinner-text {\n",
              "                    padding-left: 6px;\n",
              "                }\n",
              "            </style>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "            const element = document.getElementById(\"434c8b45-b178-4456-ae56-4ed00f158b24-circle\");\n",
              "            element.style.border = \"3px solid limegreen\";\n",
              "            element.style.animation = \"none\";\n",
              "\n",
              "            const text = document.getElementById(\"434c8b45-b178-4456-ae56-4ed00f158b24-text\");\n",
              "            text.innerText = \"Initialized Chromedriver\";\n",
              "        "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scraping page 1...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 2...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 3...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 4...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 5...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 6...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 7...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 8...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 9...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 10...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 11...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 12...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 13...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 14...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 15...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 16...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 17...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 18...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 19...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 20...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 21...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 22...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 23...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 24...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 25...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 26...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 27...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 28...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 29...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 30...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 31...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 32...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 33...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 34...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 35...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 36...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 37...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 38...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 39...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 40...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 41...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 42...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 43...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 44...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 45...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 46...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 47...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 48...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 49...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 50...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 51...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 52...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 53...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 54...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 55...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 56...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 57...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 58...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 59...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 60...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 61...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 62...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 63...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 64...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 65...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 66...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 67...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 68...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 69...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 70...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 71...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 72...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 73...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 74...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 75...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 76...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 77...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 78...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 79...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 80...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 81...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 82...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 83...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 84...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 85...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 86...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 87...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 88...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 89...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 90...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 91...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 92...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 93...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 94...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 95...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 96...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 97...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 98...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 99...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 100...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 101...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 102...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 103...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 104...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 105...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 106...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 107...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 108...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 109...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 110...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 111...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 112...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 113...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 114...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 115...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 116...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 117...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 118...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 119...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 120...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 121...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 122...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 123...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 124...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 125...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 126...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 127...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 128...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 129...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 130...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 131...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 132...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 133...\n",
            "Scraped 5 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 134...\n",
            "Scraped 4 links from the current page.\n",
            "Navigated to the next page.\n",
            "Scraping page 135...\n",
            "Scraped 0 links from the current page.\n",
            "Error navigating to the next page: Message: \n",
            "Stacktrace:\n",
            "#0 0x5ad62d1717ca <unknown>\n",
            "#1 0x5ad62cc692f0 <unknown>\n",
            "#2 0x5ad62ccb9035 <unknown>\n",
            "#3 0x5ad62ccb9251 <unknown>\n",
            "#4 0x5ad62ccff054 <unknown>\n",
            "#5 0x5ad62ccdd9dd <unknown>\n",
            "#6 0x5ad62ccfc3b3 <unknown>\n",
            "#7 0x5ad62ccdd753 <unknown>\n",
            "#8 0x5ad62ccaabaa <unknown>\n",
            "#9 0x5ad62ccabdfe <unknown>\n",
            "#10 0x5ad62d13c38b <unknown>\n",
            "#11 0x5ad62d140307 <unknown>\n",
            "#12 0x5ad62d128e7c <unknown>\n",
            "#13 0x5ad62d140ec7 <unknown>\n",
            "#14 0x5ad62d10d24f <unknown>\n",
            "#15 0x5ad62d1602f8 <unknown>\n",
            "#16 0x5ad62d1604c0 <unknown>\n",
            "#17 0x5ad62d170646 <unknown>\n",
            "#18 0x7ae7278a5ac3 <unknown>\n",
            "\n",
            "No more pages to scrape or error in finding the next page button.\n"
          ]
        }
      ],
      "source": [
        "# Initialize WebDriver\n",
        "def init_driver():\n",
        "    return gs.Chrome()\n",
        "\n",
        "driver = init_driver()\n",
        "\n",
        "# Scrape a single page for links\n",
        "def scrape_page(driver, output_file):\n",
        "    links = driver.find_elements(By.CSS_SELECTOR, 'a.MuiTypography-root.MuiTypography-subtitle2.MuiTypography-alignLeft.MuiLink-root.MuiLink-underlineHover.css-15k6acr')\n",
        "    with open(output_file, mode='a', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "        for link in links:\n",
        "            title = link.text.strip()\n",
        "            url = link.get_attribute('href')\n",
        "            writer.writerow([title, url])\n",
        "    print(f\"Scraped {len(links)} links from the current page.\")\n",
        "\n",
        "# Navigate to the next page\n",
        "def go_to_next_page(driver):\n",
        "    try:\n",
        "        next_page_button = WebDriverWait(driver, 10).until(\n",
        "            EC.element_to_be_clickable((By.CSS_SELECTOR, 'button[aria-label=\"Go to next page\"]'))\n",
        "        )\n",
        "        driver.execute_script(\"arguments[0].scrollIntoView(true);\", next_page_button)\n",
        "        driver.execute_script(\"arguments[0].click();\", next_page_button)\n",
        "        print(\"Navigated to the next page.\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Error navigating to the next page: {e}\")\n",
        "        return False\n",
        "\n",
        "# Main scraping loop\n",
        "def scrape_links():\n",
        "    url = \"https://www.lg.com/us/support/help-library\"\n",
        "    driver.get(url)\n",
        "\n",
        "    output_file = os.path.join(project_folder, \"lg_help_links.csv\")\n",
        "    with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([\"Title\", \"Link\"])\n",
        "\n",
        "    page_number = 1\n",
        "    while True:\n",
        "        print(f\"Scraping page {page_number}...\")\n",
        "        scrape_page(driver, output_file)\n",
        "        page_number += 1\n",
        "        if not go_to_next_page(driver):\n",
        "            print(\"No more pages to scrape or error in finding the next page button.\")\n",
        "            break\n",
        "        time.sleep(3)\n",
        "\n",
        "    driver.quit()\n",
        "\n",
        "# Start scraping\n",
        "scrape_links()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q19OGTxIedjL"
      },
      "outputs": [],
      "source": [
        "file_path = \"/content/drive/MyDrive/Colab Notebooks/WebscrapingProject/lg_help_links.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Remove duplicate links\n",
        "df_unique = df.drop_duplicates(subset='Link')\n",
        "\n",
        "# Save the changes back to the same CSV file\n",
        "df_unique.to_csv(file_path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import google_colab_selenium as gs\n",
        "from selenium.webdriver.common.by import By\n",
        "import time\n",
        "\n",
        "# Initialize WebDriver\n",
        "def initialize_driver():\n",
        "    return gs.Chrome()\n",
        "\n",
        "# Function to process a given DataFrame of links\n",
        "def process_links(df, output_csv, classes_to_check):\n",
        "    # Add columns for each class in `classes_to_check`\n",
        "    for class_name in classes_to_check:\n",
        "        df[class_name] = False  # Default to False\n",
        "\n",
        "    # Initialize the WebDriver\n",
        "    driver = initialize_driver()\n",
        "\n",
        "    try:\n",
        "        for index, row in df.iterrows():\n",
        "            driver.get(row['Link'])\n",
        "            time.sleep(5)\n",
        "            try:\n",
        "                article_left = driver.find_element(By.CLASS_NAME, \"article-left\")\n",
        "                li_elements = article_left.find_elements(By.CLASS_NAME, \"article-topic\")\n",
        "                theme_text = \", \".join([li.text.strip() for li in li_elements if li.text.strip()])\n",
        "                df.at[index, \"Theme\"] = theme_text\n",
        "            except Exception as e:\n",
        "                print(f\"Error extracting 'Theme': {e}\")\n",
        "                df.at[index, \"Theme\"] = \"\"\n",
        "            for class_name in classes_to_check:\n",
        "                try:\n",
        "                    elements = driver.find_elements(By.CLASS_NAME, class_name)\n",
        "                    df.at[index, class_name] = bool(elements)  # True if elements are found\n",
        "                except Exception as e:\n",
        "                    print(f\"Error checking class '{class_name}': {e}\")\n",
        "\n",
        "    finally:\n",
        "        driver.quit()  # Close the driver\n",
        "\n",
        "    # Save the updated DataFrame to a CSV\n",
        "    df.to_csv(output_csv, index=False)\n",
        "    print(f\"Data saved to {output_csv}\")\n",
        "    return df\n",
        "# Input file path\n",
        "input_csv = \"/content/drive/MyDrive/Colab Notebooks/WebscrapingProject/lg_help_links.csv\"\n",
        "\n",
        "# Read the input file\n",
        "print(f\"Reading input file: {input_csv}\")\n",
        "input_data = pd.read_csv(\n",
        "    input_csv,\n",
        "    on_bad_lines=\"skip\"  # Skip rows with parsing errors.\n",
        ")\n",
        "\n",
        "# Split the data into two halves\n",
        "mid_point = len(input_data) // 2\n",
        "df_part1 = input_data.iloc[:mid_point].copy()  # First half\n",
        "df_part2 = input_data.iloc[mid_point:].copy()  # Second half\n",
        "\n",
        "print(f\"Data split into two halves. First half: {len(df_part1)} rows, Second half: {len(df_part2)} rows.\")\n",
        "# Output CSV for the first half\n",
        "output_csv_part1 = \"/content/drive/MyDrive/Colab Notebooks/WebscrapingProject/lg_help_links_processed_part1.csv\"\n",
        "\n",
        "# Classes to check on the webpag\n",
        "classes_to_check = [\n",
        "    \"lgeai-error-code-item\"\n",
        "]\n",
        "\n",
        "# Process the first half of the data\n",
        "df_part1 = process_links(df_part1, output_csv_part1, classes_to_check)"
      ],
      "metadata": {
        "id": "wqmqYVanUPoc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "00cd66bc-4194-4221-8501-c91e13344065"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading input file: /content/drive/MyDrive/Colab Notebooks/WebscrapingProject/lg_help_links.csv\n",
            "Data split into two halves. First half: 334 rows, Second half: 335 rows.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "            <div class=\"spinner-container\">\n",
              "                <div class=\"spinner\" id=\"265b29ac-5436-4eac-bd18-28d52edb9465-circle\"></div>\n",
              "                <div class=\"spinner-text\" id=\"265b29ac-5436-4eac-bd18-28d52edb9465-text\">Initializing Chromedriver</div>\n",
              "            </div>\n",
              "            <style>\n",
              "                @keyframes spin {\n",
              "                    from { transform: rotate(0deg); }\n",
              "                    to { transform: rotate(360deg); }\n",
              "                }\n",
              "\n",
              "                .spinner-container {\n",
              "                    display: flex;\n",
              "                    align-items: center;\n",
              "                    margin-bottom: 3px;\n",
              "                }\n",
              "\n",
              "                .spinner {\n",
              "                    border: 3px solid rgba(0, 0, 0, 0.1);\n",
              "                    border-left-color: lightblue;\n",
              "                    border-radius: 50%;\n",
              "                    width: 12px;\n",
              "                    height: 12px;\n",
              "                    animation: spin 1s linear infinite;\n",
              "                }\n",
              "\n",
              "                .spinner-text {\n",
              "                    padding-left: 6px;\n",
              "                }\n",
              "            </style>\n",
              "        "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "            const element = document.getElementById(\"265b29ac-5436-4eac-bd18-28d52edb9465-circle\");\n",
              "            element.style.border = \"3px solid limegreen\";\n",
              "            element.style.animation = \"none\";\n",
              "\n",
              "            const text = document.getElementById(\"265b29ac-5436-4eac-bd18-28d52edb9465-text\");\n",
              "            text.innerText = \"Initialized Chromedriver\";\n",
              "        "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data saved to /content/drive/MyDrive/Colab Notebooks/WebscrapingProject/lg_help_links_processed_part1.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Output CSV for the second half\n",
        "output_csv_part2 = \"/content/drive/MyDrive/Colab Notebooks/WebscrapingProject/lg_help_links_processed_part2.csv\"\n",
        "\n",
        "# Process the second half of the data\n",
        "df_part2 = process_links(df_part2, output_csv_part2, classes_to_check)\n"
      ],
      "metadata": {
        "id": "oytaRXvMW74D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "outputId": "3be855a3-6af3-4606-fcf9-323436a4d910"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "            <div class=\"spinner-container\">\n",
              "                <div class=\"spinner\" id=\"6e828ac0-bbcb-457b-93d8-c2c35f79639e-circle\"></div>\n",
              "                <div class=\"spinner-text\" id=\"6e828ac0-bbcb-457b-93d8-c2c35f79639e-text\">Initializing Chromedriver</div>\n",
              "            </div>\n",
              "            <style>\n",
              "                @keyframes spin {\n",
              "                    from { transform: rotate(0deg); }\n",
              "                    to { transform: rotate(360deg); }\n",
              "                }\n",
              "\n",
              "                .spinner-container {\n",
              "                    display: flex;\n",
              "                    align-items: center;\n",
              "                    margin-bottom: 3px;\n",
              "                }\n",
              "\n",
              "                .spinner {\n",
              "                    border: 3px solid rgba(0, 0, 0, 0.1);\n",
              "                    border-left-color: lightblue;\n",
              "                    border-radius: 50%;\n",
              "                    width: 12px;\n",
              "                    height: 12px;\n",
              "                    animation: spin 1s linear infinite;\n",
              "                }\n",
              "\n",
              "                .spinner-text {\n",
              "                    padding-left: 6px;\n",
              "                }\n",
              "            </style>\n",
              "        "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "            const element = document.getElementById(\"6e828ac0-bbcb-457b-93d8-c2c35f79639e-circle\");\n",
              "            element.style.border = \"3px solid limegreen\";\n",
              "            element.style.animation = \"none\";\n",
              "\n",
              "            const text = document.getElementById(\"6e828ac0-bbcb-457b-93d8-c2c35f79639e-text\");\n",
              "            text.innerText = \"Initialized Chromedriver\";\n",
              "        "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "ReadTimeoutError",
          "evalue": "HTTPConnectionPool(host='localhost', port=38103): Read timed out. (read timeout=120)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0;31m# Get the response from http.client.HTTPConnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m         \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1394\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1395\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1396\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    717\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTimeoutError\u001b[0m: timed out",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mReadTimeoutError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-2934c37745ad>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Process the second half of the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf_part2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_links\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_part2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_csv_part2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses_to_check\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-5702099bf2bd>\u001b[0m in \u001b[0;36mprocess_links\u001b[0;34m(df, output_csv, classes_to_check)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Link'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://example.com\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m         \"\"\"\n\u001b[0;32m--> 454\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"url\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    425\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sessionId\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/selenium/webdriver/remote/remote_connection.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, command, params)\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0mtrimmed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trim_large_entries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s %s %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommand_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrimmed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/selenium/webdriver/remote/remote_connection.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, method, url, body)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeep_alive\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m             \u001b[0mstatuscode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/_request_methods.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, fields, headers, json, **urlopen_kw)\u001b[0m\n\u001b[1;32m    141\u001b[0m             )\n\u001b[1;32m    142\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             return self.request_encode_body(\n\u001b[0m\u001b[1;32m    144\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0murlopen_kw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/_request_methods.py\u001b[0m in \u001b[0;36mrequest_encode_body\u001b[0;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0mextra_kw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murlopen_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mextra_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/poolmanager.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m         \u001b[0mredirect_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mredirect\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_redirect_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    839\u001b[0m                 \u001b[0mnew_e\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProtocolError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Connection aborted.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_e\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m             retries = retries.increment(\n\u001b[0m\u001b[1;32m    842\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_e\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_pool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/util/retry.py\u001b[0m in \u001b[0;36mincrement\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0;31m# Read retry?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_method_retryable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m                 \u001b[0mread\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/util/util.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# type: ignore[assignment]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    788\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_raise_timeout\u001b[0;34m(self, err, url, timeout_value)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSocketTimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m             raise ReadTimeoutError(\n\u001b[0m\u001b[1;32m    368\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Read timed out. (read timeout={timeout_value})\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m             ) from err\n",
            "\u001b[0;31mReadTimeoutError\u001b[0m: HTTPConnectionPool(host='localhost', port=38103): Read timed out. (read timeout=120)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Final output file\n",
        "final_output_csv = \"/content/drive/MyDrive/Colab Notebooks/WebscrapingProject/lg_help_links_processed_final.csv\"\n",
        "\n",
        "# Combine the two parts\n",
        "df_part1 = pd.read_csv(output_csv_part1)\n",
        "df_part2 = pd.read_csv(output_csv_part2)\n",
        "\n",
        "# Concatenate the DataFrames\n",
        "df_final = pd.concat([df_part1, df_part2], ignore_index=True)\n",
        "\n",
        "# Save the combined DataFrame to a CSV\n",
        "df_final.to_csv(final_output_csv, index=False)\n",
        "print(f\"Final processed data saved to {final_output_csv}\")\n"
      ],
      "metadata": {
        "id": "B-BQMehDXHQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input file path\n",
        "input_csv = \"/content/drive/MyDrive/Colab Notebooks/WebscrapingProject/lg_help_links.csv\"\n",
        "\n",
        "# Read the input file\n",
        "print(f\"Reading input file: {input_csv}\")\n",
        "input_data = pd.read_csv(input_csv)\n",
        "\n",
        "# Split the data into two halves\n",
        "mid_point = len(input_data) // 2\n",
        "df_part1 = input_data.iloc[:mid_point].copy()  # First half\n",
        "df_part2 = input_data.iloc[mid_point:].copy()  # Second half\n",
        "\n",
        "print(f\"Data split into two halves. First half: {len(df_part1)} rows, Second half: {len(df_part2)} rows.\")\n"
      ],
      "metadata": {
        "id": "FEMIbOAfWw3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Specify the path to the CSV file\n",
        "csv_file_path = '/content/drive/MyDrive/Colab Notebooks/WebscrapingProject/lg_help_links_processed.csv'\n",
        "\n",
        "# Load the CSV file into a DataFrame\n",
        "df = pd.read_csv(csv_file_path)\n",
        "\n",
        "# Check for initial data overview\n",
        "print(\"Initial data overview:\")\n",
        "print(df.head())\n",
        "\n",
        "# Create a boolean mask where 'Content' contains the 'Link' value\n",
        "print(\"Creating a mask to identify rows where 'Link' is in 'Content'...\")\n",
        "mask = df.apply(lambda row: str(row['Link']) in str(row['Content']), axis=1)\n",
        "\n",
        "# Drop rows where the mask is True\n",
        "df_cleaned = df[~mask]\n",
        "\n",
        "# Save the cleaned DataFrame to a new CSV\n",
        "cleaned_csv_path = '/content/drive/MyDrive/Colab Notebooks/WebscrapingProject/data_cleaned.csv'\n",
        "df_cleaned.to_csv(cleaned_csv_path, index=False)\n",
        "\n",
        "# Print the cleaned DataFrame or check the first few rows\n",
        "print(f\"Cleaned data saved to {cleaned_csv_path}\")\n",
        "print(\"Cleaned data overview:\")\n",
        "print(df_cleaned.head())\n"
      ],
      "metadata": {
        "id": "cHpuX_rjkmyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Folder to save images\n",
        "image_folder = \"/content/drive/MyDrive/Colab Notebooks/WebscrapingProject/images\"\n",
        "os.makedirs(image_folder, exist_ok=True)\n",
        "\n",
        "# Function to initialize the selenium driver\n",
        "def init_driver():\n",
        "    print(\"Initializing WebDriver...\")\n",
        "    driver = gs.Chrome()  # Correct method to start the WebDriver\n",
        "    return driver\n",
        "\n",
        "# Function to sanitize filenames\n",
        "def sanitize_filename(filename):\n",
        "    return re.sub(r'[\\/:*?\"<>|]', '_', filename)\n",
        "\n",
        "# Function to extract data based on element class\n",
        "def extract_element_text(driver, element_class, multiple=False, attribute=None):\n",
        "    elements = driver.find_elements(By.CLASS_NAME, element_class)\n",
        "    if multiple:\n",
        "        return [el.get_attribute(attribute) if attribute else el.text.strip() for el in elements if el]\n",
        "    return elements[0].text.strip() if elements else None\n",
        "\n",
        "# Function to scrape data from a page\n",
        "def scrape_site(url, driver=None):\n",
        "    output_data = {\n",
        "        \"Theme\": None,\n",
        "        \"Category\": None,\n",
        "        \"Subclass_Category\": []\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        driver.get(url)\n",
        "        time.sleep(2)  # Allow page to load\n",
        "\n",
        "        # Extract Theme\n",
        "        output_data[\"Theme\"] = extract_element_text(driver, \"article-topic\")\n",
        "\n",
        "        # Extract Category\n",
        "        output_data[\"Category\"] = extract_element_text(driver, \"article-category\")\n",
        "\n",
        "        # Extract Subclass Category\n",
        "        subclass_elements = driver.find_elements(By.CLASS_NAME, \"lgeai-box-hover\")\n",
        "        for element in subclass_elements:\n",
        "            aria_label = element.get_attribute(\"aria-label\")\n",
        "            if aria_label:\n",
        "                output_data[\"Subclass_Category\"].append(aria_label)\n",
        "\n",
        "        # Extract and group text content\n",
        "        text_elements = driver.find_elements(By.CLASS_NAME, \"lgeai-description\")\n",
        "        text_grouped = [\", \".join([text.strip() for text in el.text.split('\\n')]) for el in text_elements]\n",
        "\n",
        "        # Extract video links\n",
        "        video_links = [\n",
        "            anchor.get_attribute(\"href\")\n",
        "            for anchor in driver.find_elements(By.TAG_NAME, \"a\")\n",
        "            if \"youtu.be\" in (anchor.get_attribute(\"href\") or \"\")\n",
        "        ]\n",
        "\n",
        "        # Download images\n",
        "        image_data = []\n",
        "        img_tags = driver.find_elements(By.CSS_SELECTOR, \"div.img-only img\")\n",
        "        for img in img_tags:\n",
        "            img_url = img.get_attribute(\"src\")\n",
        "            alt_text = img.get_attribute(\"alt\") or \"default_image\"\n",
        "            if img_url and alt_text:\n",
        "                img_filename = os.path.join(image_folder, f\"{sanitize_filename(alt_text)}.jpg\")\n",
        "                response = requests.get(img_url, stream=True)\n",
        "                if response.status_code == 200:\n",
        "                    with open(img_filename, \"wb\") as f:\n",
        "                        for chunk in response.iter_content(1024):\n",
        "                            f.write(chunk)\n",
        "                    image_data.append({\"alt_text\": alt_text, \"img_url\": img_url, \"img_filename\": img_filename})\n",
        "\n",
        "        output_data[\"Content\"] = {\n",
        "            \"text_grouped\": text_grouped,\n",
        "            \"video_links\": video_links,\n",
        "            \"images\": image_data\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping {url}: {e}\")\n",
        "\n",
        "    return output_data\n",
        "\n",
        "# Save data as JSON file\n",
        "def save_data_as_json(data, output_filename):\n",
        "    output_json_filename = \"/content/drive/MyDrive/Colab Notebooks/WebscrapingProject/output_data.json\"\n",
        "    with open(output_json_filename, 'w') as json_file:\n",
        "        json.dump(data, json_file, indent=4)\n",
        "    print(f\"Data saved as {output_json_filename}\")\n",
        "\n",
        "# Main function to process URLs\n",
        "def process_urls():\n",
        "    # Path to the CSV file containing URLs\n",
        "    csv_file = \"/content/drive/MyDrive/Colab Notebooks/WebscrapingProject/data_cleaned.csv\"\n",
        "\n",
        "    # Initialize WebDriver once\n",
        "    driver = init_driver()\n",
        "\n",
        "    # Load data from CSV file\n",
        "    df = pd.read_csv(csv_file)\n",
        "    urls = df[\"Content\"].dropna().tolist()  # Fetch links from the 'Content' column\n",
        "\n",
        "    all_data = []\n",
        "\n",
        "    # Loop through each URL and scrape data\n",
        "    for index, row in df.iterrows():\n",
        "        url = row[\"Content\"]\n",
        "        question = row[\"Title\"]\n",
        "        error_code = row[\"Error Code\"]\n",
        "\n",
        "        print(f\"Scraping URL: {url}\")\n",
        "        scraped_data = scrape_site(url, driver=driver)\n",
        "\n",
        "        # Prepare the final output data\n",
        "        data = {\n",
        "            \"Question\": question,\n",
        "            \"Theme\": scraped_data[\"Theme\"],\n",
        "            \"Category\": scraped_data[\"Category\"],\n",
        "            \"Subclass_Category\": scraped_data[\"Subclass_Category\"],\n",
        "            \"Error Code\": error_code,\n",
        "            \"Content\": scraped_data[\"Content\"]\n",
        "        }\n",
        "\n",
        "        all_data.append(data)\n",
        "\n",
        "    # Save all the data as a JSON file\n",
        "    save_data_as_json(all_data, \"output_data.json\")\n",
        "\n",
        "    # Quit the WebDriver once done\n",
        "    driver.quit()\n",
        "\n",
        "# Run the processing function\n",
        "process_urls()\n"
      ],
      "metadata": {
        "id": "uUPM65qCnhBN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO2HzkjZE6CaFmUKuNDiNlD",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}